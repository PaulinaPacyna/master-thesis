
%\begin{thebibliography}{50} % IF YOU HAVE MORE REFERENCES, WRITE THE BIGGER NUMBER
%
%\bibitem[1]{bake_off}Bagnall, A., Lines, J., Bostrom, A. et al. The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data Min Knowl Disc 31, 606–660 (2017). \url{https://doi.org/10.1007/s10618-016-0483-9}
%\bibitem[2]{dl_tsc} Ismail Fawaz, H., Forestier, G., Weber, J. et al. Deep learning for time series classification: a review. Data Min Knowl Disc 33, 917–963 (2019). \url{https://doi.org/10.1007/s10618-019-00619-1}
%\bibitem[3]{imagnet} Huh, Minyoung \& Agrawal, Pulkit \& Efros, Alexei. (2016). What makes ImageNet good for transfer learning?. 
%\bibitem[4]{survey_transfer_learning} Weiss, K., Khoshgoftaar, T.M. \& Wang, D. A survey of transfer learning. J Big Data 3, 9 (2016). \url{https://doi.org/10.1186/s40537-016-0043-6} 
%
%
@book{bake_off,
  doi = {10.48550/ARXIV.1602.01711},
  
  url = {https://arxiv.org/abs/1602.01711},
  
  author = {Bagnall, Anthony and Bostrom, Aaron and Large, James and Lines, Jason},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{dl_tsc,
	doi = {10.1007/s10618-019-00619-1},
  
	url = {https://doi.org/10.1007%2Fs10618-019-00619-1},
  
	year = 2019,
	month = {mar},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {33},
  
	number = {4},
  
	pages = {917--963},
  
	author = {Hassan Ismail Fawaz and Germain Forestier and Jonathan Weber and Lhassane Idoumghar and Pierre-Alain Muller},
  
	title = {Deep learning for time series classification: a review},
  
	journal = {Data Mining and Knowledge Discovery}
}

@book{imagnet,
  doi = {10.48550/ARXIV.1608.08614},
  
  url = {https://arxiv.org/abs/1608.08614},
  
  author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {What makes ImageNet good for transfer learning?},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@book{survey_transfer_learning, 
  doi = {https://doi.org/10.1186/s40537-016-0043-6},
  
  publisher = {Journal of Big Data}, 
  
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
  
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
 
  title = {A survey of transfer learning.},
   
  
  year = {2016},
 
}

@book{comp_survey_transfer_leaerning,
  doi = {10.48550/ARXIV.1911.02685},
  
  url = {https://arxiv.org/abs/1911.02685},
  
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Comprehensive Survey on Transfer Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{deep_tranfer_learning,
  doi = {10.48550/ARXIV.1808.01974},
  
  url = {https://arxiv.org/abs/1808.01974},
  
  author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Survey on Deep Transfer Learning},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{encoder,
  doi = {10.48550/ARXIV.1805.03908},
  
  url = {https://arxiv.org/abs/1805.03908},
  
  author = {Serrà, Joan and Pascual, Santiago and Karatzoglou, Alexandros},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards a universal neural network encoder for time series},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{attention,
  doi = {10.48550/ARXIV.1409.0473},
  
  url = {https://arxiv.org/abs/1409.0473},
  
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{residual,
  doi = {10.48550/ARXIV.1611.06455},
  
  url = {https://arxiv.org/abs/1611.06455},
  
  author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{transfer_learning_time_series,
	doi = {10.1109/bigdata.2018.8621990},
  
	url = {https://doi.org/10.1109%2Fbigdata.2018.8621990},
  
	year = 2018,
	month = {dec},
  
	publisher = {{IEEE}
},
  
	author = {Hassan Ismail Fawaz and Germain Forestier and Jonathan Weber and Lhassane Idoumghar and Pierre-Alain Muller},
  
	title = {Transfer learning for time series classification},
  
	booktitle = {2018 {IEEE} International Conference on Big Data (Big Data)}
}
@article{dtw_dba,
title = {A global averaging method for dynamic time warping, with applications to clustering},
journal = {Pattern Recognition},
volume = {44},
number = {3},
pages = {678-693},
year = {2011},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2010.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S003132031000453X},
author = {François Petitjean and Alain Ketterlin and Pierre Gançarski},
keywords = {Sequence analysis, Time series clustering, Dynamic time warping, Distance-based clustering, Time series averaging, DTW barycenter averaging, Global averaging, Satellite image time series},
abstract = {Mining sequential data is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. Most works in this field are centred on the definition and use of a distance (or, at least, a similarity measure) between sequences of elements. A measure called dynamic time warping (DTW) seems to be currently the most relevant for a large panel of applications. This article is about the use of DTW in data mining algorithms, and focuses on the computation of an average of a set of sequences. Averaging is an essential tool for the analysis of data. For example, the K-means clustering algorithm repeatedly computes such an average, and needs to provide a description of the clusters it forms. Averaging is here a crucial step, which must be sound in order to make algorithms work accurately. When dealing with sequences, especially when sequences are compared with DTW, averaging is not a trivial task. Starting with existing techniques developed around DTW, the article suggests an analysis framework to classify averaging techniques. It then proceeds to study the two major questions lifted by the framework. First, we develop a global technique for averaging a set of sequences. This technique is original in that it avoids using iterative pairwise averaging. It is thus insensitive to ordering effects. Second, we describe a new strategy to reduce the length of the resulting average sequence. This has a favourable impact on performance, but also on the relevance of the results. Both aspects are evaluated on standard datasets, and the evaluation shows that they compare favourably with existing methods. The article ends by describing the use of averaging in clustering. The last section also introduces a new application domain, namely the analysis of satellite image time series, where data mining techniques provide an original approach.}
}
@misc{multiscale,
  doi = {10.48550/ARXIV.1603.06995},
  
  url = {https://arxiv.org/abs/1603.06995},
  
  author = {Cui, Zhicheng and Chen, Wenlin and Chen, Yixin},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-Scale Convolutional Neural Networks for Time Series Classification},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{ lenet,
        title={ Data Augmentation for Time Series Classification using Convolutional Neural Networks },
        authors={ Arthur Le Guennec and Simon Malinowski and Romain Tavenard },
        
        year={ 2016 },
        }

%\end{thebibliography}