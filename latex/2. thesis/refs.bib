@book{bake_off,
  doi = {10.48550/ARXIV.1602.01711},
  url = {https://arxiv.org/abs/1602.01711},
  author = {Bagnall, Anthony and Bostrom, Aaron and Large, James and Lines, Jason},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Great Time Series Classification Bake Off: An Experimental Evaluation of Recently Proposed Algorithms. Extended Version},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{dl_tsc,
  doi = {10.1007/s10618-019-00619-1},
  url = {https://doi.org/10.1007%2Fs10618-019-00619-1},
  year = 2019,
  month = {mar},
  publisher = {Springer Science and Business Media {LLC}},
  volume = {33},
  number = {4},
  pages = {917--963},
  author = {Hassan Ismail Fawaz and Germain Forestier and Jonathan Weber and Lhassane Idoumghar and Pierre-Alain Muller},
  title = {Deep learning for time series classification: a review},
  journal = {Data Mining and Knowledge Discovery}
}
@book{imagnet,
  doi = {10.48550/ARXIV.1608.08614},
  url = {https://arxiv.org/abs/1608.08614},
  author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {What makes ImageNet good for transfer learning?},
  publisher = {arXiv},
  year = {2016},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@book{survey_transfer_learning,
  doi = {https://doi.org/10.1186/s40537-016-0043-6},
  publisher = {Journal of Big Data},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  title = {A survey of transfer learning.},
  year = {2016},
}
@book{comp_survey_transfer_leaerning,
  doi = {10.48550/ARXIV.1911.02685},
  url = {https://arxiv.org/abs/1911.02685},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Comprehensive Survey on Transfer Learning},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{deep_tranfer_learning,
  doi = {10.48550/ARXIV.1808.01974},
  url = {https://arxiv.org/abs/1808.01974},
  author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Survey on Deep Transfer Learning},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{encoder,
  doi = {10.48550/ARXIV.1805.03908},
  url = {https://arxiv.org/abs/1805.03908},
  author = {Serrà, Joan and Pascual, Santiago and Karatzoglou, Alexandros},
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Towards a universal neural network encoder for time series},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{attention,
  doi = {10.48550/ARXIV.1409.0473},
  url = {https://arxiv.org/abs/1409.0473},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{tsc_scratch,
  doi = {10.48550/ARXIV.1611.06455},
  url = {https://arxiv.org/abs/1611.06455},
  author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{transfer_learning_time_series,
  doi = {10.1109/bigdata.2018.8621990},
  url = {https://doi.org/10.1109%2Fbigdata.2018.8621990},
  year = 2018,
  month = {dec},
  publisher = {IEEE},
  author = {Hassan Ismail Fawaz and Germain Forestier and Jonathan Weber and Lhassane Idoumghar and Pierre-Alain Muller},
  title = {Transfer learning for time series classification},
  booktitle = {2018 {IEEE} International Conference on Big Data (Big Data)}
}
@article{rocket,
  author       = {Angus Dempster and Fran{\c{c}}ois Petitjean and Geoffrey I. Webb},
  title        = {{ROCKET:} Exceptionally fast and accurate time series classification using random convolutional kernels},
  journal      = {CoRR},
  volume       = {abs/1910.13051},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.13051},
  eprinttype   = {arXiv},
  eprint       = {1910.13051},
  timestamp    = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-13051.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{inceptiontime,
  author       = {Hassan Ismail Fawaz and Benjamin Lucas and Germain Forestier and Charlotte Pelletier and Daniel F. Schmidt and Jonathan Weber and Geoffrey I. Webb and Lhassane Idoumghar and Pierre{-}Alain Muller and Fran{\c{c}}ois Petitjean},
  title        = {InceptionTime: Finding AlexNet for Time Series Classification},
  journal      = {CoRR},
  volume       = {abs/1909.04939},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.04939},
  eprinttype   = {arXiv},
  eprint       = {1909.04939},
  timestamp    = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-04939.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{dtw_dba,
  title = {A global averaging method for dynamic time warping, with applications to clustering},
  journal = {Pattern Recognition},
  volume = {44},
  number = {3},
  pages = {678-693},
  year = {2011},
  issn = {0031-3203},
  doi = {https://doi.org/10.1016/j.patcog.2010.09.013},
  url = {https://www.sciencedirect.com/science/article/pii/S003132031000453X},
  author = {François Petitjean and Alain Ketterlin and Pierre Gançarski},
  keywords = {Sequence analysis, Time series clustering, Dynamic time warping, Distance-based clustering, Time series averaging, DTW barycenter averaging, Global averaging, Satellite image time series},
  abstract = {Mining sequential data is an old topic that has been revived in the last decade, due to the increasing availability of sequential datasets. Most works in this field are centred on the definition and use of a distance (or, at least, a similarity measure) between sequences of elements. A measure called dynamic time warping (DTW) seems to be currently the most relevant for a large panel of applications. This article is about the use of DTW in data mining algorithms, and focuses on the computation of an average of a set of sequences. Averaging is an essential tool for the analysis of data. For example, the K-means clustering algorithm repeatedly computes such an average, and needs to provide a description of the clusters it forms. Averaging is here a crucial step, which must be sound in order to make algorithms work accurately. When dealing with sequences, especially when sequences are compared with DTW, averaging is not a trivial task. Starting with existing techniques developed around DTW, the article suggests an analysis framework to classify averaging techniques. It then proceeds to study the two major questions lifted by the framework. First, we develop a global technique for averaging a set of sequences. This technique is original in that it avoids using iterative pairwise averaging. It is thus insensitive to ordering effects. Second, we describe a new strategy to reduce the length of the resulting average sequence. This has a favourable impact on performance, but also on the relevance of the results. Both aspects are evaluated on standard datasets, and the evaluation shows that they compare favourably with existing methods. The article ends by describing the use of averaging in clustering. The last section also introduces a new application domain, namely the analysis of satellite image time series, where data mining techniques provide an original approach.}
}
@misc{multiscale,
  doi = {10.48550/ARXIV.1603.06995},
  url = {https://arxiv.org/abs/1603.06995},
  author = {Cui, Zhicheng and Chen, Wenlin and Chen, Yixin},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Multi-Scale Convolutional Neural Networks for Time Series Classification},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{ timelenet,
  TITLE = {{Data Augmentation for Time Series Classification using Convolutional Neural Networks}},
  AUTHOR = {Le Guennec, Arthur and Malinowski, Simon and Tavenard, Romain},
  URL = {https://shs.hal.science/halshs-01357973},
  BOOKTITLE = {{ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data}},
  ADDRESS = {Riva Del Garda, Italy},
  YEAR = {2016},
  MONTH = Sep,
  KEYWORDS = {time series ; convolutional neural networks},
  PDF = {https://shs.hal.science/halshs-01357973/file/AALTD16_paper_9.pdf},
  HAL_ID = {halshs-01357973},
  HAL_VERSION = {v1}
  }
@article{UCR_article,
  author    = {Hoang Anh Dau and Anthony J. Bagnall and Kaveh Kamgar and Chin{-}Chia Michael Yeh and Yan Zhu and Shaghayegh Gharghabi and Chotirat Ann Ratanamahatana and Eamonn J. Keogh},
  title     = {The {UCR} Time Series Archive},
  journal   = {CoRR},
  volume    = {abs/1810.07758},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.07758},
  eprinttype = {arXiv},
  eprint    = {1810.07758},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-07758.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{UCR_archive,
  title = {The UCR Time Series Classification Archive},
  author = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan
  and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing
  and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo and Hexagon-ML},
  year = {2018},
  month = {October},
  note = {\url{https://www.cs.ucr.edu/~eamonn/time_series_data_2018/}}
}
@misc{residual,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year={2015},
  eprint={1512.03385},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
  @article{hivecote,
  author       = {Matthew Middlehurst and James Large and Michael Flynn and Jason Lines and Aaron Bostrom and Anthony J. Bagnall},
  title        = {{HIVE-COTE} 2.0: a new meta ensemble for time series classification},
  journal      = {CoRR},
  volume       = {abs/2104.07551},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.07551},
  eprinttype    = {arXiv},
  eprint       = {2104.07551},
  timestamp    = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-07551.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{finetuning,
  author={Vrbančič, Grega and Podgorelec, Vili},
  journal={IEEE Access},
  title={Transfer Learning With Adaptive Fine-Tuning},
  year={2020},
  volume={8},
  pages={196197-196211},
  doi={10.1109/ACCESS.2020.3034343}}
@article{first_tl,
  author = {Bozinovski, Stevo},
  year = {2020},
  month = {09},
  title = {Reminder of the First Paper on Transfer Learning in Neural Networks, 1976},
  volume = {44},
  journal = {Informatica},
  doi = {10.31449/inf.v44i3.2828}
}
@InProceedings{unsup_tl,
  title = 	 {Unsupervised and Transfer Learning Challenge: a Deep Learning Approach},
  author = 	 {Mesnil, Grégoire and Dauphin, Yann and Glorot, Xavier and Rifai, Salah and Bengio, Yoshua and Goodfellow, Ian and Lavoie, Erick and Muller, Xavier and Desjardins, Guillaume and Warde-Farley, David and Vincent, Pascal and Courville, Aaron and Bergstra, James},
  booktitle = 	 {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  pages = 	 {97--110},
  year = 	 {2012},
  editor = 	 {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
  volume = 	 {27},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bellevue, Washington, USA},
  month = 	 {02 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v27/mesnil12a/mesnil12a.pdf},
  url = 	 {https://proceedings.mlr.press/v27/mesnil12a.html},
  abstract = 	 {Learning good representations from a large set of unlabeled data is a particularly challenging task. Recent work (see Bengio (2009) for a review) shows that training deep architectures is a good way to extract such representations, by extracting and disentangling gradually higher-level factors of variation characterizing the input distribution. In this paper, we describe different kinds of layers we trained for learning representations in the setting of the Unsupervised and Transfer Learning Challenge. The strategy of our team won the final phase of the challenge. It combined and stacked different one-layer unsupervised learning algorithms, adapted to each of the five datasets of the competition. This paper describes that strategy and the particular one-layer learning algorithms feeding a simple linear classifier with a tiny number of labeled training samples (1 to 64 per class).}
}
@InProceedings{unsup_tl_2,
  title = 	 {Deep Learning of Representations for Unsupervised and Transfer Learning},
  author = 	 {Bengio, Yoshua},
  booktitle = 	 {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  pages = 	 {17--36},
  year = 	 {2012},
  editor = 	 {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
  volume = 	 {27},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bellevue, Washington, USA},
  month = 	 {02 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v27/bengio12a/bengio12a.pdf},
  url = 	 {https://proceedings.mlr.press/v27/bengio12a.html},
  abstract = 	 {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution $P(x)$ is structurally related to some task of interest, say predicting $P(y|x)$. This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.}
}
@article{selflearning,
  author = {Triguero, Isaac and García, Salvador and Herrera, Francisco},
  year = {2015},
  month = {02},
  title = {Self-labeled techniques for semi-supervised learning: Taxonomy, software and empirical study},
  volume = {42},
  journal = {Knowledge and Information Systems},
  doi = {10.1007/s10115-013-0706-y}
}
@article{dtw,
author = {Senin, Pavel},
year = {2009},
month = {01},
title = {Dynamic Time Warping Algorithm Review}
}
