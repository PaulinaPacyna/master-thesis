\documentclass[a4paper,11pt,twoside]{report}
% THIS FILE SHOULD BE COMPILED BY pdfLaTeX

% ----------------------   PREAMBLE PART ------------------------------

% ------------------------ ENCODING & LANGUAGES ----------------------

\usepackage[utf8]{inputenc}
%\usepackage[MeX]{polski} % Not needed unless You have a name with polish symbols or sth
\usepackage[T1]{fontenc}
\usepackage[english, polish]{babel}

\usepackage{placeins}

\usepackage{amsmath, amsfonts, amsthm, latexsym} % MOSTLY MATHEMATICAL SYMBOLS

\usepackage[final]{pdfpages} % INPUTING TITLE PDF PAGE - GENERATE IT FIRST!
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}


\usepackage{commath} % various commands which can make writing math expressions easier --- documentation available at: https://ctan.gust.org.pl/tex-archive/macros/latex/contrib/commath/commath.pdf

\usepackage[hidelinks]{hyperref} % for hyperlinks, for example, urls, references to equations, entries in a bibliography --- hidelinks option removes rectangles around hiperlinks


% ---------------- MARGINS, INDENTATION, LINESPREAD ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry} % MARGINS


\linespread{1.5}
\allowdisplaybreaks         % ALLOWS BREAKING PAGE IN MATH MODE

\usepackage{indentfirst}    % IT MAKES THE FIRST PARAGRAPH INDENTED; NOT NEEDED
\setlength{\parindent}{5mm} % WIDTH OF AN INDENTATION


%---------------- RUNNING HEAD - CHAPTER NAMES, PAGE NUMBERS ETC. -------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% PAGINATION: LEFT ALIGNMENT ON EVEN PAGES, RIGHT ALIGNMENT ON ODD PAGES 
\fancyfoot[LE,RO]{\thepage} 
% RIGHT HEADER: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ #1}{}}

% HEAD RULE - IT'S A LINE WHICH SEPARATES HEADER AND FOOTER FROM CONTENT
\renewcommand{\headrulewidth}{0 pt} % 0 MEANS NO RULE, 0.5 MEANS FINE RULE, THE BIGGER VALUE THE THICKER RULE


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[LE,RO]{\thepage}
  
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0.0pt}
}



% --------------------------- CHAPTER HEADERS ---------------------

\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 

    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- TABLE OF CONTENTS SETUP ---------------------------

\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% THIS MAKES DOTS IN TOC FOR CHAPTERS
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- TABLES AD FIGURES NUMBERING ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}


% ------------- DEFINING ENVIRONMENTS FOR THEOREMS, DEFINITIONS ETC. ---------------

\makeatletter
\newtheoremstyle{definition}
{3ex}%                           % Space above
{3ex}%                           % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                          % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\makeatother

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}

% --------------------- END OF PREAMBLE PART (MOSTLY) --------------------------





% -------------------------- USER SETTINGS ---------------------------

\newcommand{\tytul}{Zastosowanie techniki transfer learning w zadaniu klasyfikacji szeregów czasowych}
\renewcommand{\title}{Transfer learning for time series classification}
\newcommand{\type}{Master} % Master OR Engineer
\newcommand{\supervisor}{Agnieszka Jastrzębska, PhD} % TITLE AND NAME OF THE SUPERVISOR

%---------------------------Moje ustawienia-----------------------------

\DeclareMathOperator{\real}{\mathbb{R}}
\DeclareMathOperator{\natur}{\mathbb{N}}

\setlength{\headheight}{14.0pt}
\usepackage{listings}
\renewcommand{\arraystretch}{0.7}
 \usepackage{booktabs}
%\setlength\parskip{\baselineskip}
%\usepackage{parskip}
%--------------------------Start---------------------------------------
\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage-en} % THIS INPUTS THE TITLE PAGE

\null\thispagestyle{empty}\newpage

% ------------------ PAGE WITH SIGNATURES --------------------------------

%\thispagestyle{empty}\newpage
%\null
%
%\vfill
%
%\begin{center}
%\begin{tabular}[t]{ccc}
%............................................. & \hspace*{100pt} & .............................................\\
%supervisor's signature & \hspace*{100pt} & author's signature
%\end{tabular}
%\end{center}
%


% ---------------------------- ABSTRACTS -----------------------------

{  \fontsize{12}{14} \selectfont
\begin{abstract}

\begin{center}
\title
\end{center}
The task of classifying time series is an important problem in the field of data mining. Time series occur every time we want to measure some phenomenon that changes over time. A time series can describe for example the amplitude of a heartbeat sound, stock prices or hand movement along an axis when serving in tennis. Such time series can express different characteristics of the phenomenon. Those characteristics are called \textit{classes}. For example, the heartbeat sound amplitude can represent (belong to) one of two classes: \textit{healthy} and \textit{unhealthy}. Time series classification attempts to learn the distinctive features of a time series and build a model that can distinguish between those classes.

The time series classification problem was initially solved using classical algorithms such as the k-nearest neighbor classifier with distance measures suited for time series, like Dynamic Time Warping. Still, the advantages of using deep learning algorithms in the context of time series classification have recently begun to be recognized. Neural networks are capable of detecting shapes that distinguish a class or understanding ordered temporal relationships. 

Transfer learning practically is used when there is limited data to train. Transfer learning attempts to apply patterns learned from one dataset to improve learning when creating a model for another dataset. A common practice is to prepare a source classifier trained on a large, easily available amount of data for one task and then use this model or parts of it for a detailed task with a smaller amount of data. Models trained using this method often have a shorter training time, faster accuracy increase, and can generalize more easily on the test set.

\noindent \textbf{Keywords:} time series, classification, transfer learning, deep learning, ...
\end{abstract}
}

\null\thispagestyle{empty}\newpage


{\selectlanguage{polish} \fontsize{12}{14}\selectfont
\begin{abstract}

\begin{center}
\tytul
\end{center}

Zadanie klasyfikacji szeregów czasowych jest ważnym problemem w dziedzinie eksploracji danych. Szeregi czasowe występują za każdym razem, gdy chcemy zmierzyć jakieś zjawisko, które zmienia się w czasie. Szereg czasowy może opisywać np. amplitudę dźwięku bicia serca, ceny akcji czy ruch ręki wzdłuż osi podczas serwu w tenisie. Takie szeregi czasowe mogą wyrażać różne cechy zjawiska. Te cechy nazywane są \textit{klasami}. Na przykład amplituda dźwięku bicia serca może reprezentować (należeć do) jednej z dwóch klas:  \textit{norma} i \textit{choroba}. Klasyfikacja szeregów czasowych polega na rozpoznawaniu charakterystycznych cech szeregu czasowego i budowanie modelu, który potrafi rozróżniać klasy.

Problem klasyfikacji szeregów czasowych był początkowo rozwiązywany za pomocą klasycznych algorytmów, takich jak algorytm k-najbliższych sąsiadów w połączeniu z miarami podobieństwa dla szeregów, jak odległość DTW. W ostatnim czasie zaczęto dostrzegać zalety stosowania algorytmów głębokiego uczenia w kontekście klasyfikacji szeregów czasowych. Sieci neuronowe są w stanie wykryć kształty wyróżniające daną klasę lub zrozumieć relacje między obserwacjami w czasie. 

Metoda transfer learning jest stosowane w przypadku ograniczonej ilości danych do tranowania modelu. Technika transfer learning próbuje zastosować wzorce wyuczone z jednego zbioru danych, aby poprawić uczenie podczas tworzenia modelu dla innego zbioru danych. Częstą praktyką jest przygotowanie źródłowego klasyfikatora wytrenowanego na dużej, łatwo dostępnej ilości danych dla jednego zadania, a następnie wykorzystanie tego modelu lub jego części do szczegółowego zadania z mniejszą ilością danych. Modele wytrenowane tą metodą często mają krótszy czas trenowania, szybszy wzrost dokładności i lepsze, ogólniejsze wyniki na zbiorze testowym.

 

\noindent \textbf{Słowa kluczowe:} szeregi czasowe, klasyfikacja, transfer learning ...
\end{abstract}
}


%% --------------------------- DECLARATIONS ------------------------------------
%
%%
%%	IT IS NECESSARY OT ATTACH FILLED-OUT AUTORSHIP DEECLRATION. SCAN (IN PDF FORMAT) NEEDS TO BE PLACED IN scans FOLDER AND IT SHOULD BE CALLED, FOR EXAMPLE, DECLARATION_OF_AUTORSHIP.PDF. IF THE FILENAME OR FILEPATH IS DIFFERENT, THE FILEPATH IN THE NEXT COMMAND HAS TO BE ADJUSTED ACCORDINGLY.
%%
%%	command attacging the declarations of autorship
%%
%\includepdf[pages=-]{scans/declaration-of-autorship}
%\null\thispagestyle{empty}\newpage
%
%% optional declaration
%%
%%	command attaching the declaataration on granting a license
%%
%\includepdf[pages=-]{scans/declaration-on-granting-a-license}
%%
%%	.tex corresponding to the above PDF files are present in the 3. declarations folder 
%
\null\thispagestyle{empty}\newpage
% ------------------- TABLE OF CONTENTS ---------------------
% \selectlanguage{english} - for English
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}
\newpage % IF YOU HAVE EVEN QUANTITY OD PAGES OF TOC, THEN REMOVE IT OR ADD \null\newpage FOR DOUBLE BLANK PAGE BEFORE INTRODUCTION


% -------------------- THE BODY OF THE THESIS --------------------------------

\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11}


\chapter*{Introduction}
%\markboth{}{Introduction}
%\addcontentsline{toc}{chapter}{Introduction}
Time series classification was initially approached using classical machine learning algorithms. The paper \cite{bake_off} categorizes the commonly used algorithms into several categories. The first category are time domain distance based algorithms. Those algorithms use various distance measures adjusted for time series domain to capture similarity between pairs of time series. The distance measure is then combined with a distance-based classifier. A flagship example of an algorithm belonging to this class of algorithms is Dynamic Time Warping distance with k-Nearest Neighbour classifier, often used as a benchmark classifier. Another category of classifiers mentioned in \cite{bake_off} are dictionary, shapelet and interval based classifiers. All of them try to extract features distinctive for the class of the time series. The dictionary based classifiers encode the time series into a dictionary of \textit{words} representing the time series. Shapelet based algorithms focus on subseries of the time series that are discriminatory of class membership. Interval based algorithms extract features from selected intevals of the time series. With the rise of popularity of deep learning algorithms researchers made an attempt to replace the former, hand-extracted features with deep learning classifiers. A few years after the article \cite{dl_tsc} was published. It contains a review of deep learning algorithms applied to the task of time series classification. The usage of deep learning algorithms enabled the possibility to utilize transfer learning. 

Transfer learning is widely used in image recognition and natural language processing.  The authors of paper \cite{dl_tsc} extended their previous finding by a study on application of transfer learning. The authors examine knowledge transerability on 85 dataset from UCR archive, by pre-training a model on one dataset and fine-tuning on another. The authors examine if the accuracy improved for all pairs of datasets in UCR archive.

Transfer learning for deep learning is usually done by training the network on a big, diverse dataset and utilizing first layers of the network when training on another dataset. In image recognition or natural language processing it is common to pre-train the source network on ImageNet dataset or Wikipedia dataset respectively. Such diverse, dataset with labels does not exists for time series. In this thesis, we will attempt to create such dataset from 85 smaller datasets available on the UCR archive. We will try to mimic good properties of a transfer learning source dataset by preprocessing, upsampling and augmenting the dataset. We will also study it transfer learning on a diverse helps to generalize to new training data and improve the training process on the target dataset. Similarly as in \cite{imagnet}, we will experiment to find which features of the source dataset (classes diversity, dataset size, augmentation, preprocessing) are fundamental for the transfer learning process.
%What is the thesis about? What is the content of it? What is the Author's contribution to it?
%\par
%WARNING!  In a diploma thesis which is a team project: Description of the work division in the team, including the scope of each co-author’s contribution to the practical part (Team Programming Project) and the descriptive part of the diploma thesis. 
%\par
\chapter{Related works}
In this chapter, we describe several algorithms used in time series classification. We will also recall theoretical definitions used to describe transfer learning. 
\section{UCR archive}
The URC (University of California, Riverside) archive in a time series classification archive introduced in 2002 by Dau, Keogh et al \cite{UCR_archive}. At the moment of writing this thesis, it consists of 128 univariate time series datasets. The time series describe various phenomenons, like readings from a device or sensor, motion recordings or food spectrographs. The summary of types of the time series is listed below (Table \ref{table:ucr_summary}). We also show a few samples of the time series from different datasets below (Figure \ref{fig:UCR_samples}).
\begin{table}[!h]
\centering
\tabcolsep=0.11cm
\scalebox {0.9}{
\begin{tabular}{@{}|l|l|@{}}
%\begin{singlespace}
\toprule
\textbf{Dataset type (source)}       & \textbf{Number of datasets} \\ \midrule
Device                               & 9                           \\ \midrule
ECG (Electrocardiogram)              & 6                           \\ \midrule
EOG (Electrooculography)             & 2                           \\ \midrule
EPG (Electrical penetration   graph) & 2                           \\ \midrule
Hemodynamics                         & 3                           \\ \midrule
HRM (High Resolution Melt)           & 1                           \\ \midrule
Image                                & 32                          \\ \midrule
Motion                               & 17                          \\ \midrule
Power                                & 1                           \\ \midrule
Sensor                               & 30                          \\ \midrule
Simulated                            & 8                           \\ \midrule
Spectro                              & 8                           \\ \midrule
Spectrum                             & 4                           \\ \midrule
Traffic                              & 2                           \\ \midrule
Trajectory                           & 3                           \\ \bottomrule
%\end{singlespace}
\end{tabular}
}
\caption{Summary of types of datasets in the UCR archive.}
\label{table:ucr_summary}
\end{table}
\FloatBarrier

\begin{figure}[h!]
\centering
\includegraphics[height=7cm]{imgs/UCR_Beef.png} 
\includegraphics[height=7cm]{imgs/UCR_ECG200.png} 
\includegraphics[height=7cm]{imgs/UCR_Yoga.png} 
\includegraphics[height=7cm]{imgs/UCR_FreezerRegularTrain.png} 
\caption{Example time series from the UCR archive. The images belong to the following categories: Food, ECG, Image, Device}
\label{fig:UCR_samples}
\end{figure}
All datasets together contain 1118 unique labels. Most of the datasets differentiate between two labels and have several dozen of rows per class. The series length vary from 8 to almost 3 thousands. Below we show four histograms summarizing number of classes per dataset, lengths of the series and number of observations per class and per dataset. (Figure \ref{fig:histograms})
\begin{figure}[h!]
\centering
\includegraphics[width=7.9cm]{imgs/lengths_of_the_series_in_the_datasets.png} 
\includegraphics[width=7.9cm]{imgs/number_of_classes_per_dataset.png} 
\includegraphics[width=7.9cm]{imgs/number_of_observations_per_class.png} 
\includegraphics[width=7.9cm]{imgs/number_of_observations_per_dataset.png} 
\caption{Summary of the UCR archive. The first plot is a histogram of time series length. The second histogram shows number of classes in datasets. The two last histograms summarize number of rows per class or per dataset}
\label{fig:histograms}
\end{figure}
The datasets in the archive are already splitted in train and test datasets.  Most of the time series are already z-normalized.  However the authors recommend testing on additional test/train splits and preprocessing the time series accordingly to the method used.

The authors of \cite{UCR_article} composed a set of best practices for the dataset users when conducting experiments on the time series archive. The first of recommendation is to avoid \textit{cherry-picking}, which is testing the method on only a subset on the time series. It is recommended to report the results on all datasets. The second recommendation is to tune hyperparameters using only train dataset. Another recommendation is to avoid claiming that the improvement is caused by a certain feature of the algorithm without extensive tests of the feature. To illustrate why it is important, the authors bring up an example of attaching accuracy improvement in classifiers based on wavelet transform to the multi-resolution property of the transformation. The authors show, that in certain cases, the accuracy gain is caused by the fact that wavelet transform is implicitly smoothing the time series, thus the classifier could be improved by simply smoothing the time series. The authors recommend checking that removing the feature worsen the performance or comparing against a different method but with the same property.


\section{Time series classification}
A time series is an ordered collection of observations indexed by time. 
$$X = (x_t)_{t\in T} = (x_1, ... , x_T),\ x_t\in \real$$
The time index $T$ can represent any collection with the natural order. We assume that indices are spaced evenly in the set $T$. The realization or observation $x_t$ in the times series is a numerical value describing the phenomena we observe, for example, the amplitude of a sound, stock price, or y-coordinate. Time series classification is a problem of finding the optimal mapping between a set of time series and corresponding classes.

%Time series classification can be formulated as follows:
%Given a dataset  $D = {(X_i, y_i)}_{i=1}^N$, where $X_i \in 2^\real$ is a time series and $y_i\in {1, ..., N} = C$, we would like to find a function (called classifier) $f_\theta :2^\real \leftarrow C$ such that 
%$$f_\theta (X_i) = y_i$$. 

\subsection{Dynamic Time Warping with k-Nearest Neighbour}
The Dynamic Time Warping \cite{bake_off} with k-Nearest Neighbour classifier uses a distance-based algorithm with a specific distance measure. A DTW distance between time series $X^1$, $X^2$ of equal lengths is defined as follows:
$$DTW(X^1, X^2) = \min\{ \sum_{i=1}^S dist(x^1_{e_i}, x^2_{f_i}):(e_i)_{i=1}^S, (f_i)_{i=1}^S\in 2^T\}$$
subject to:
\begin{itemize}
\item $e_1 = 1, f_1 = 1, e_S= N, f_S = N$
\item $|e_{i+1} - e_i | \leq 1$, $|f_{i+1} - f_i | \leq 1$
\end{itemize}
The measure defined above, used in the k-Nearest Neighbour classifier, is often used as a benchmark classifier. 
The list of indices $[(e_1, f_1), \dots, (e_S, f_S)]$ is called the warping path. An illustration of this measure is displayed on figure

\FloatBarrier

\ref{fig:dtw_img}
\begin{figure}
\centering
\includegraphics[width=10cm]{imgs/dtw.jpg}
\caption{The Dynamic Time warping distance between the series above is equal to 1. We show the warping path in the distance matrix and connections indicated by the dynamic warping path.}
\label{fig:dtw_img}
\end{figure}

\FloatBarrier

\subsection{Multi Layer Perceptron}
The Multi Layer Perceptron (MLP) is the first artificial neural network architecture proposed in \cite{dl_tsc} and can be used for time series classification task. 
%A MLP network can be formally defined as a function depending on a set \textit{weights} $\theta_i$. 
%$$MLP(X; \theta_1,\dots , \theta_M, \beta_1,\dots , \beta_M) = f_M( \beta_M +  \theta_M \cdot (f_{M-1}( \beta_{M-1} +  \theta_{M-1} \cdot (\dots f_1( \beta_1 +  \theta_1 \cdot X)))))$$
Formally, the MLP network can be defined as a composition of functions (called \textit{dense layers} or \textit{layers}). The network returns a vector that usually represents the probability distribution over the set of classes.
$$MLP(X; \theta_1,\dots , \theta_M, \beta_1,\dots , \beta_M) = L_M(\dots L_2(L_1(X;\theta_1, \beta_1);\theta_2, \beta_2);\theta_M, \beta_M)$$
Each layer $L_i: \real^M \rightarrow \real^N$ is a function that depends on the parameters $\theta \in r^{M\times N}, \beta \in \real^N$
$$L_i(X ; \theta_i, \beta_i) = f_i(X \theta_i  + \beta_i)$$
The function $f_i: \real^N \rightarrow \real^N$ is an arbitrarily chosen non-linear function. The number of layers and dimension of weights in hidden layers are also arbitrary. The weights in the first and last layer have to match the dimensionality of input data (e.g. the length of the time series) and the number of classes. The output of the last layer is interpreted as a probability distribution over the set of classes. 

The disadvantage of using Multi Layer Perceptrons for time series classification is that the input size is fixed. All time series is the training data must have the same length. In transfer learning, this means that if we want to reuse the source network (or a set of first layers from the network), the time series in the target dataset must have the same length as in the source dataset. 

The MLP architecture fails at understanding the temporal dependencies \cite{dl_tsc}. Each input value in the time series is treated separately because it is multiplied by a separate row in the weight matrix. 

\subsection{Convolutional Neural Networks}
Convolutional Neural Networks are widely used in image recognition. A convolution applied for a time series can be interpreted as sliding a filter over the time series. A convolutional layer is a set of functions called convolutions or filters. The filter is applied at a given point, taking into account the values that surround the point. 

To define the convolution operation, let's assume the input is a matrix $X \in \real^{(N_1, \dots, N_K)}$. In the case of images, the number of dimensions $K$ is often equal to $3$ (height, width, channels), for univariate time series we can assume just one dimension, and for multivariate time series we need two dimensions - (feature, time). 
The filter consists of a matrix of weights $M \in \real^{(P_1, \dots, P_K)}$. 
Usually, $P_l$ are odd numbers, so that we can index the matrix with symmetrical numbers: $ (\frac{-P_l+1}{2},  \frac{-P_l+3}{2}, \dots, 0, \dots, \frac{P_l-1}{2})$. The $0$ index marks the center of the matrix. 
%TODO - powolac sie czemu nieparzyscie

Finally the convolution $*$ is defined as follows:
$$(X*M)_{i_1, \dots, i_K} = \sum_{l_1=\frac{-P_1+1}{2}}^{\frac{P_1-1}{2}} \cdots \sum_{l_K=\frac{-P_K+1}{2}}^{\frac{P_K-1}{2}} M_{l_1, \dots, l_K} X_{i_1 + l_1, \dots, i_K + l_K}$$

The result of the convolution is passed elementwise to a nonlinear function. The nonlinear function together with the convolution operation will be called a filter.

In the case of univariate time series, the first layer of the convolutional neural network is one-dimensional. The output of the first layer has dimensions (length of time series - the length of the filter + 1, number of filters). Below we define the value of the output for filter $i$  
$$ y_{t, i} = f_i([\theta_{\frac{-M+1}{2}}^i, \dots , \theta_{\frac{M-1}{2}}^i] \cdot [X_{t_+\frac{-M+1}{2}}, \dots, X_{t+\frac{M-1}{2}}]),$$ 
where $\cdot$ is a dot product and $t \in T$ is the time index.
In figures and we show results of applying two different filter to a time series.


\FloatBarrier

\begin{figure}[h!t]
\centering
\includegraphics[width=16cm]{imgs/convolution_output_growth.png}
\caption{Result of applying slope detecting filter to a time series. Red regions indicate higher value of the output convolution.}
\label{fig:convolution_output_growth}
\end{figure}
\FloatBarrier
\begin{figure}[h!]
\centering
\includegraphics[width=16cm]{imgs/convolution_output_hills.png}
\caption{Result of applying a "peak" detecting filter to a time series. Red regions indicate higher value of the output convolution.}
\label{fig:convolution_output_hill}
\end{figure}

\FloatBarrier

The weights $\theta^i$ are different for each filter.
The same filter is applied over the whole length of the time series. This is called \textit{weight sharing} and it enables the network to learn patterns regardless of the position in the time series. 

The architecture of the convolutional layer is not dependent on the size of the input data. Regardless of the size of the input data, the number of filters and size of filters remain the same, only the output sizes depend on the input size. Therefore, if the convolutional layer is  succeeded by layers with the same property, like other convolutional layers or Global Pooling with Dense Layer (see section \ref{FCN}), the whole network may be invariant to the input sizes \cite{dl_tsc}. Such networks may be interesting in terms of transfer learning, as the sizes of time series in the source task and in the target task do not have to match.  

\subsection{Fully Convolutional Networks}\label{FCN}
Fully Convolutional Networks are convolutional networks used in time series classification. A sample architecture of a Fully Convolutional Network proposed is in \cite{dl_tsc}. The first layers in the network are 3 blocks of convolutional layers with ReLU activation function followed by batch normalization layers. The output of the last block is passed to a global pooling layer. The global pooling layer averages the output through the time axis, resulting in a vector of length equal to the number of feature maps in the last convolutional layer. The averaged vector is passed to a block of 2 fully connected layers. Figure \ref{fig:FCN_img} shows a visualisation of the network.



\FloatBarrier

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{imgs/FCN.png}
\caption{Architecture of a Fully Convolutional Network. Source: \cite{dl_tsc}}
\label{fig:FCN_img}
\end{figure}

\FloatBarrier

Because the architecture of convolutional layers does not depend on the size of input data and the convolutional layers are followed by pooling over the time axis, the whole network is capable of processing data of variable lengths.
\subsection{Residual Network}
The Residual Network was first proposed in \cite{residual}. The Residual Network is a relatively deep architecture compared to other neural networks used for time series classification. 

A residual connection addresses the vanishing gradient problem occuring in networks composed of many layers \cite{residual}. The vanishing gradient problem occurs in the backpropagation process. When the gradient is passes from the last layer to the first, it may be decreasing towards zero. This causes the first layers of the network to learn slowly. The residual connection between layers passes the input directly from one layer to another, skipping a few layers. This way if the network is struggling with vanishing gradient, this shortcut connection may help the network to converge. 

The architecture of the Residual Network is conceptually similar to the FCN network. Instead of three convolutional layers, the Residual Network begins with three blocks of convolutional layers, connected with residual connections. Each block consists of three convolutional layers. The three blocks, which consist of nine convolutional layers and three residual connections in total, are followed by a Global Average Pooling layer and a Dense layer. The networks' diagram is shown below (Figure \ref{fig:Resnet_img}).


\FloatBarrier

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{imgs/resnet.png}
\caption{Architecture of a Residual Network. Source: \cite{dl_tsc}}
\label{fig:Resnet_img}
\end{figure}

\FloatBarrier

\subsection{Encoder Network}
 The Encoder Network is a deep convolutional network proposed by \cite{encoder}. The first layers of the network are similar to the FCN architecture. The network consists of three convolutional layer followed by an attention layer instead of the Global Average Pooling layer. Another novelty introduced in this network comparing with the FCN architecture is adding a DropOut leayer and replacing the ReLU activation function with PReLU (Parametrized ReLU, thus adding another parameter to the network. The network ends with a Dense layer predicting the distribution over the classes. The architecture is shown below (Figure \ref{fig:encoder_img}).
 
\FloatBarrier

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{imgs/encoder.png}
\caption{Architecture of an Encoder Network. Source: \cite{dl_tsc}}
\label{fig:encoder_img}
\end{figure}

\FloatBarrier

The attention layer, first proposed in \cite{attention}, assigns weights to each element of the input representing the importance for the prediciton. The weights representing the attention are normalized using softmax function and then applied on the input data. The weigths  are learned by the network. The outputs of the attention layer can be interpreted as a universal representation of the input time series, that will be able to adapt to and represent unseen data \cite{encoder}.
 
As opposed to the FCN network architecture, the Encoder network is not invariant to the input size. The Dense layer parameters depend on the output size of the attention layer and implicilitely on the output size of convolutional layers.  Similarly as former architecture, the convolutional layers enable weight sharing thus learning shapelets independently of their position in the time series. 

\section{Preprocessing and augmenting time series}
Data preprocessing is the process of preparing, cleansing and manipulating the data in order to optimize the training process. Data augmentation is a process used to increase the amount and variability of data. Data augmentation is beneficial when there is limited amount of data, as it can prevent overfitting.
A lot of classical and deep learning models were invented and designed together with the data preprocessing and augmenting step \cite{bake_off, dl_tsc}. An example of such method is Window Slicing (\cite{dl_tsc}). This method extracts subsequences of of the original series. The resulting subsequences are then concatenenated with a downsampled copy and a copy that was smoothed. This method is used in Multi-scale Convolutional Neural Network (\cite{multiscale}) and Time Le-Net (\cite{timelenet}).

Another preprocessing and augmentation method proposed together with the former method in Time Le-Net model was Window Warping. It aims at making the model more robust to perturbations in the time axis, by training the network on \textit{squeezed} or \textit{dilated} series together with the original series. The \textit{dilated}/\textit{squeezed} time series is two times longer/shorter than the original time series. It is extracted from the original time series by linear interpolation. %TODO napewno?
The dilated, squezzed and original time series are concatenated, forming a vector 3.5 longer than the original time series. In the Time Le-Net model this vector undergoes the Window Slicing preprocessing step before training.

Data augmentation may be also achieved as a by-product of Generative Adversarial Networks. TimeGAN network is an example of Generative Adversarial Network suitable for processing time series. 
% TODO szczegoły

\section{Transfer learning}
Transfer learning is a technique that attempts to apply knowledge learned  while solving one task to enhance the learning process for another task. Formally, the problem can be described using the notions of tasks and domains \cite{survey_transfer_learning, comp_survey_transfer_leaerning}. A \textit{Domain} is a pair $\mathcal{D}=(\mathcal{X}, P(\mathcal{X}))$, where $\mathcal{X}$ is the feature space (e.g. the time series observations, and $P(\mathcal{X})$ is the probability distribution over the feature space. A \textit{Task} is a pair of label space $\mathcal{Y}$ and the decision function $f$, $\mathcal{T} = (\mathcal{Y}, f)$. The decision function $f$ is learned from $\mathcal{X}, P(\mathcal{X}), \mathcal{Y}$ in the learning process.

Transfer learning attempts to utilize knowledge  domain/domains and task/tasks. Formally, given $S \in \natur$ source domains and source tasks ($\{(\mathcal{D}_i^S,\mathcal{T}_i^S): i=1, \dots, S \}$) and $T \in \natur$ target domains and target tasks ($\{(\mathcal{D}_i^T,\mathcal{T}_i^T): i=1, \dots, S \}$) transfer learning utilizes knowledge learned from source domains and tasks to improve the learning process of decision functions in target tasks $\mathcal{T}_i^T$
\subsection{Transfer learning categorization}
Transfer learning can be categorized from different points of view \cite{comp_survey_transfer_leaerning}. One of the ways in which we can divide transfer learning algorithms is based on the availability of the labels:
\begin{itemize}
\item \textbf{inductive} transfer learning - labels are available for both source and target datasets
\item \textbf{transductive} transfer learning - labels are available only in the domain dataset
\item \textbf{unsupervised} transfer learning - no labels available in either dataset
\end{itemize}
Another way of dividing transfer learning methods is by comparing the distribution of feature space and labels. If the source and training dataset consists of the same features, belonging to a similar distribution ($\mathcal{D^S} = \mathcal{(X, P(X))^S} = \mathcal{(X, P(X))^T} = \mathcal{D^T})$) and the label spaces are equal ($\mathcal{Y^S} = \mathcal{Y^T} $), then the task can be described as homogeneous transfer learning. If at least one of the former assumptions does not hold, the transfer learning setting is called heterogeneous. 

Transfer learning can be categorized based on the algorithm/approach used. There are four main categories \cite{deep_tranfer_learning} used in deep learning:
\begin{itemize}
	\item \textbf{instance} based transfer learning - target dataset is enriched by instance from the source dataset belonging to the target distribution.
	\item \textbf{mapping} based transfer learning - source and target dataset are mapped to one domain. The distribution is adjusted in both datasets.
	\item \textbf{network} based transfer learning - target classifier uses parts of the network from the source classifier $f$.
	\item \textbf{adversarial} based transfer learning - an adversarial network tries to distinguish between the two datasets. If the network fails at this task, then it means that the datasets are similar and the netowork extracted features similar between source and target datasets. 
\end{itemize}

\subsection{Characteristics of a good source domain}

In the field of image processing, it is very common to use convolutional neural networks \mbox{pre-trained} with the ImageNet dataset \cite{imagnet}.  ImageNet is a large dataset of human-annotated images. It contains 1 million labeled images of 1000 classes. The label space consists of fine-grained classes such as breeds of dogs and cats, but also coarse-grained classes like \textit{red wine} and \textit{traffic light}. Example pictures from this dataset are shown below \. As transfer learning based on this dataset became more popular and successful, a question arose: Which features of this dataset make it so good for this task?
\FloatBarrier

\begin{figure}[h!]
\centering
\includegraphics[height=5cm]{imgs/lamp.jpeg} 
\includegraphics[height=5cm]{imgs/bird1.jpeg} 
\includegraphics[height=5cm]{imgs/bird2.jpeg} 
\caption{Sample images from the ImageNet dataset, with examples of very similar (fine-grained) classes like two birds of different breeds and coarse-grained classes, like a lamp and a bird. \href{https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data}{Source: url}}
\label{fig:image_net}
\end{figure}

\FloatBarrier

A study conducted in \cite{imagnet} attempts to answer this question. The first hypothesis is that the volume of the dataset is relevant to train accurate, general classifiers. The authors compared models pretrained on the original dataset and models based on sampled subsets (reduced 2, 4 8 and 20 times). The results show that the more training examples, the better results. The accuracy of the initial classifier occurred to be more dependent on the size of dataset than the accuracy of classifiers fine-tuned from the base classifier. It is natural, that the base classifier's accuracy will depend on the dataset size. Still, the classifier fine-tuned from the base classifier seems to cope with the reduced dataset and the impact on the accuracy is less detrimental. This is visible on the figure \ref{fig:size_acc_img}.

\FloatBarrier


\begin{figure}[h!]
\centering
\includegraphics[width=10cm]{imgs/imagenet_instances_accuracy.png}
\caption{Accuracy of the base classifier (black) and classifiers fine-tuned from the base classifier. Image source: \cite{imagnet}}
\label{fig:size_acc_img}
\end{figure}
\FloatBarrier

Next experiments examine the label space. The authors study if the granularity of the label space is essential for the problem. To compare the results, the label space is clustered and 127 classes are derived from the initial 1000 classes. Pre-training with the reduced label space has a minimal negative impact on the accuracy of classifiers fine-tuned from this classifier. This suggests that such a fine division may not be needed.

Finally, the last question is if we train the classifier on the reduced label space with 127 classes, will it be able to distinguish between the fine-grained classes? To examine that, the authors extracted features from the first layers of the networks trained on reduced label space. Then, the authors performed classification with 1-NN and 5-NN models on the extracted feature space, but with 1000 classes. The findings are that the k-NN classifier performs $15\%$ worse on reduced dataset versus normal dataset. This shows that CNNs are capable of implicitly learning representative features distinctive between similar classes even when trained on coarse-grained classes.     

While the article \cite{imagnet} does not conclude which single feature of ImageNet dataset makes it so efficient as a source dataset for transfer learning, it is clear that all properties of this dataset are important for the accuracy of the classifier. We will try to recreate the properties of ImageNet dataset when creating the source dataset from time series.


\label{Negative_transfer}
\subsection{Negative transfer in naive transfer learning approach}
Negative transfer is a problem in the transfer learning process, when the target classifies achieves worse accuracy than without using transfer learning. Transfer learning for time series has been investigated in \cite{transfer_learning_time_series}. The authors of the paper tested transfer learning on all 85 datasets from the UCR archive. In the first approach, called \textit{naive transfer learning}, the authors used all pairs of datasets. One dataset from the pair was used as a source dataset, to prepare the source classifier for transfer learning. The second dataset was used to fine-tune the former classifier. The architecture used in this approach was the Fully Connected Network, described in section \ref{FCN}.  All layers except for the last one were transferred from the source classifier to the target classifier. The authors compared the accuracy of a finetuned classifier versus the accuracy of a classifier trained without the use of transfer learning. 

With respect to different source datasets, target classifiers' accuracies exhibited a high variance. There were cases where the fine-tuned classifiers suffered from transfer learning. On the other hand, for each target dataset there was always a source dataset benefical for the classification results. It is shown on the picture \ref{fig:tr_learning_min_max}, where for each target dataset, the maximum, median and minimum accuracies for different experiments values were plotted.

\FloatBarrier


\begin{figure}[h!]
\centering
\includegraphics[width=10cm]{imgs/max_min_transfer_learning.png}
\caption{Image source: \cite{transfer_learning_time_series}}
\label{fig:tr_learning_min_max}
\end{figure}
\FloatBarrier
A negative transfer learning effect can be minimized or avoided by optimally choosing the source dataset/datasets. To improve the transfer learning process, the authors proposed a method of choosing the source dataset based on the DTW distance. This method will be described in \ref{DTW_choosing}.

\label{DTW_choosing}
\section{Inter-dataset similarity based on Dynamic Time Warping Barycenter Averaging}
Choosing the source dataset for transfer learning randomly, or by trial and error, can lead to the \textit{Negative transfer} problem, described in the previous chapter (\ref{Negative_transfer}). In \cite{transfer_learning_time_series}, the authors proposed a inter-dataset similarity measure, that is used to optimally choose the source dataset for transfer learning. The first step of the method is to compute a \textit{prototype} time series per each class, for every dataset that is considered. This reduces the volume of the data, that will be used is the next step. The next step is to compute the distances between each pair of datasets, choosing the minimum DTW distance between prototypes of each classes in both datasets. We proceed to describe the algorithm in detail below.

The algorithm uses DTW barycenter averaging \cite{dtw_dba}.  The result of this method is a times series that minimizes the sum of DTW distances to each time series from the dataset.
$$DTW_average = \arg\min_{X \in \real^T} \sum_{\bar{X}\in \mathcal{X}} DTW(X, \bar{X})$$
The resulting time series does not need to belong to the original dataset $\mathcal(X)$.

The first step of the algorithm is the data reduction step. For each class in the dataset $\{(X_i, y_i): i=1, \dots , N\}$, $y_i=c \in \{ 1, \dots, C\}$ the prototype time series $P_c$ is defined as the result of DTW barycenter averaging performed on the subset of $\mathcal{X}$ corresponding to class $c$. In result, the original dataset is reduced to $C$ observations.


%\pagebreak
The next step is to compute the distance between target dataset and all possible source datasets. Te distance between two datasets is the minimum DTW distance between each pair of prototypes from the reduced datasets. The whole algorithm is described in detail in \cite{transfer_learning_time_series}.

In \cite{transfer_learning_time_series} the distance was computed pairwise for all available datasets (using train splits). The authors compare the transfer learning performance of with randomly choosen source dataset versus choosing the source dataset that minimizes the former distance to the target dataset. The results are shown below (Figure \ref{fig:smart_transfer_learning}).
\FloatBarrier


\begin{figure}[h!]
\centering
\includegraphics[width=10cm]{imgs/smart_transfer_learning.png}
\caption{Image source: \cite{transfer_learning_time_series}}
\label{fig:smart_transfer_learning}
\end{figure}
\FloatBarrier

The result show that choosing the source dataset for transfer learning based on similarity to the target dataset may lead to benefits. In this work we will try to construct the dataset artificially, by combining, preprocessing and augmenting the datasets from UCR archive, to mimic the properties of ImageNet dataset. The artificial dataset will be composed of multiple dataset. Therefore we rather won't use this similarity to choose one particular dataset. We will take the inter-dataset distance into account when adjusting the distribution of classes in the artificial dataset, to favour classes similar to the target dataset.
% ------------------------------- BIBLIOGRAPHY ---------------------------
% LEXICOGRAPHICAL ORDER BY AUTHORS' LAST NAMES
% FOR AMBITIOUS ONES - USE BIBTEX
\
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file


\pagenumbering{gobble}
\thispagestyle{empty}



% ----------------------- LIST OF SYMBOLS AND ABBREVIATIONS ------------------
%\chapter*{List of symbols and abbreviations}
%
%\begin{tabular}{cl}
%nzw. & nadzwyczajny \\
%* & star operator \\
%$\widetilde{}$ & tilde 
%\end{tabular}
%\\
%If you don't need it, delete it.
%\thispagestyle{empty}
%
%
%% ----------------------------  LIST OF FIGURES --------------------------------
%\listoffigures
%\thispagestyle{empty}
%If you don't need it, delete it.
%
%
%% -----------------------------  LIST OF TABLES --------------------------------
%\renewcommand{\listtablename}{Spis tabel}
%\listoftables
%\thispagestyle{empty}
%If you don't need it, delete it.
%
%% -----------------------------  LIST OF APPENDICES ---------------------------
%\chapter*{List of appendices}
%\begin{enumerate}
%\item Appendix 1
%\item Appendix 2
%\item In case of no appendices, delete this part.
%\end{enumerate}
%\thispagestyle{empty}


\end{document}
